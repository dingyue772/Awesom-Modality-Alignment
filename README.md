# Awesome Modality Alignment
Modality Alignment refers to the process of ensuring that different modalities are appropriately aligned. Specifically, in Large Vision-Language Models (LVLMs), the goal is to align visual tokens with the embedding space of the large language model.

In the LVLM's community, there're some methods to bridge the gap between visual and textual representations.

----

- **SEA**: SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs(Aug. 21, 2024)
  - [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2408.11813)

- **VLAP**: Bridging Vision and Language Spaces with Assignment Prediction(Apr. 15, 2024)
  - [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.09632) [![Star](https://img.shields.io/github/stars/park-jungin/vlap.svg?style=social&label=Star)](https://github.com/park-jungin/vlap)

- **V2T Tokenizer**: Beyond Text: Frozen Large Language Models in Visual Signal Comprehension(Mar 12, 2024)
  -  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.07874) [![Star](https://img.shields.io/github/stars/zh460045050/v2l-tokenizer.svg?style=social&label=Star)](https://github.com/zh460045050/v2l-tokenizer)


- **SoM prompting**: List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs(Apr 25, 2024)
  -  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.16375)  [![Star](https://img.shields.io/github/stars/zzxslp/som-llava.svg?style=social&label=Star)](https://github.com/zzxslp/som-llava)
- **AlignGPT**: AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability(May 23, 2024)
  - [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.14129)  [![Star](https://img.shields.io/github/stars/AlignGPT-VL/AlignGPT.svg?style=social&label=Star)](https://github.com/AlignGPT-VL/AlignGPT)

- **Visual Prompting**: Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge(Jul 5, 2024)
  - [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.04681) 

- **AC score**: Law of Vision Representation in MLLMs(Aug 29, 2024)
  - [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2408.16357) [![Star](https://img.shields.io/github/stars/bronyayang/law_of_vision_representation_in_mllms.svg?style=social&label=Star)](https://github.com/bronyayang/law_of_vision_representation_in_mllms)

- **a visual embedding highway module**: X-VILA: Cross-Modality Alignment for Large Language Model(May 29, 2024)
  - [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.19335)

- **LexVLA**: Unified Lexical Representation for Interpretable Visual-Language Alignment(Jul 25, 2024)
  - [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.17827)
